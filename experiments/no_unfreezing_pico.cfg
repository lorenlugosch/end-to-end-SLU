[experiment]
seed=1234
folder=experiments/no_unfreezing_pico
slots=intent

[phoneme_module]
use_sincnet=True
fs=16000

cnn_N_filt=80,60,60
cnn_len_filt=401,5,5
cnn_stride=80,1,1
cnn_max_pool_len=2,1,1
cnn_use_laynorm_inp=True
cnn_use_batchnorm_inp=False
cnn_use_laynorm=True,True,True
cnn_use_batchnorm=False,False,False
cnn_act=leaky_relu,leaky_relu,leaky_relu
cnn_drop=0.0,0.0,0.0

phone_rnn_num_hidden=128,128
phone_downsample_len=2,2
phone_downsample_type=avg,avg
phone_rnn_drop=0.5,0.5
phone_rnn_bidirectional=True

[word_module]
word_rnn_num_hidden=128,128
word_downsample_len=2,2
word_downsample_type=avg,avg
word_rnn_drop=0.5,0.5
word_rnn_bidirectional=True
vocabulary_size=10000

[intent_module]
intent_rnn_num_hidden=128
intent_downsample_len=1
intent_downsample_type=none
intent_rnn_drop=0.5
intent_rnn_bidirectional=True

[pretraining]
asr_path=/scratch/lugosch/librispeech
pretraining_type=2
; 0 - no pre-training, 1 - phoneme loss, 2 - word loss + phoneme loss
pretraining_lr=0.001
pretraining_batch_size=64
pretraining_num_epochs=10
pretraining_length_mean=2.25
pretraining_length_var=1

[training]
slu_path=/home/ec2-user/Fluent-Speech-Commands/data_pico/splits/
unfreezing_type=0
; 0 - no unfreezing, 1 - unfreeze word layers, 2 - unfreeze word layers and phoneme layers
training_lr=0.001
training_batch_size=64
training_num_epochs=20
real_dataset_subset_percentage=1.0
synthetic_dataset_subset_percentage=1.0
real_speaker_subset_percentage=1.0
synthetic_speaker_subset_percentage=0.0
train_wording_path=None
; path to .txt file containing phrases to be included during training; if None, uses all phrases
test_wording_path=None
